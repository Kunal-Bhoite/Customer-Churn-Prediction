# -*- coding: utf-8 -*-
"""CustomerChurn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ljj9xlmUwxoThC4lYPlTQEgLkFg0mIBs
"""

#Data preparation

from pandas import read_csv,get_dummies,DataFrame #importing functions from pandas library
dataset=read_csv("/content/drive/MyDrive/Colab Notebooks/CustomerChurn.csv") #we are reading our file and storing it in dataset
#print(dataset) #we are displaying our dataset
#print(dataset.info()) #we are checking the information of the dataset
#print(dataset.shape) #we are checking the shape of our dataset
print(dataset['Attrition_Flag'].unique()) #to check the unique values in this attribute
print(dataset['Gender'].unique()) #to check the unique values in this attribute
print(dataset['Education_Level'].unique()) #to check the unique values in this attribute
print(dataset['Marital_Status'].unique()) #to check the unique values in this attribute
print(dataset['Income_Category'].unique()) #to check the unique values in this attribute
print(dataset['Card_Category'].unique()) #to check the unique values in this attribute
dataset['Attrition_Flag']=dataset['Attrition_Flag'].map({'Existing Customer':1,'Attrited Customer':0}) #as there are only two unique values in this attribute so we are mapping the values in this step using map function
dataset['Gender']=dataset['Gender'].map({'M':1,'F':0}) #as there are only two unique values in this attribute so we are mapping the values in this step using map function
dataset['Education_Level']=dataset['Education_Level'].map({'Uneducated':1,'High School':2,'Graduate':3,'Post-Graduate':4,'Doctorate':5}) #as there are only two unique values in this attribute so we are mapping the values in this step using map function
dataset['Income_Category']=dataset['Income_Category'].map({'Less than $40K':1,'$40K - $60K':2,'$60K - $80K':3,'$80K - $120K':4,'$120K +':5}) #as there are only two unique values in this attribute so we are mapping the values in this step using map function
dataset['Card_Category']=dataset['Card_Category'].map({'Silver':1,'Gold':2,'Platinum':3,'Blue':0}) #as there are only two unique values in this attribute so we are mapping the values in this step using map function
print(dataset.info()) #we are checking the info of the dataset since we have converted categorical features to int
Nds=get_dummies(dataset,columns=['Marital_Status'],drop_first=True) #we are using get dummies for 'Marital_Status' since there are 3 unqiue values in the attribute so we are using one hot encoding and we are also dropping the 1st column
print(Nds.info()) #we are displaying the new info of the dataset to check weather all the features are int or are there any categorical features
X=Nds.drop('Attrition_Flag',axis=1) #we are assigned the whole dataset to X except 'Attrition_Flag' ,X is used for training
Y=Nds['Attrition_Flag'] #we have assigned output to Y,Y is the output/Test
from sklearn.preprocessing import StandardScaler #we are importing standardscaler function from sklearn library preprocessing subclass
X_scaled = StandardScaler().fit_transform(X) #we are scaling
from sklearn.model_selection import train_test_split #we are importing fit transform function from sklearn library model_selection subclass
X_train,X_test,Y_train,Y_test=train_test_split(X_scaled,Y,test_size=0.30,random_state=100) #we are splitting the dataset and training set we are giving 30% of dataset to testing and 70% for training and we are using the random records from dataset
from imblearn.over_sampling import SMOTE # Synthetic Minority Oversampling Technique is imported from imblearn library
smote = SMOTE(random_state = 101) #using SMOTE to balance the imbalanced value in dataset
X_train,Y_train = smote.fit_resample(X_train,Y_train)

"""#Random Forest Classifier using pipeline"""

from imblearn.pipeline import Pipeline #importing pipeline from imblearn
model0 = Pipeline([('balancing', SMOTE(random_state = 101)),
        ('classification', RandomForestClassifier(criterion='entropy', max_features='sqrt', random_state=100) )# building classifier
    ]) # building classifier
no_trees = {'classification__n_estimators': [10,20,30,40,50,60,70,80,90,100,110,120,130,140,150,160,170,180,190,200,210,220,230,240,250,260,270,280,290,300]}
grid_search = GridSearchCV(estimator=model0, param_grid=no_trees, scoring='precision', cv=5)
grid_search.fit(X_scaled, Y)

best_parameters = grid_search.best_params_
print(best_parameters)
best_result = grid_search.best_score_
print(best_result)

"""#Support Vector Machine"""

from imblearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
from sklearn import svm
SVM_classifier2 = Pipeline([('balancing', SMOTE(random_state = 101)),('classification', svm.SVC(random_state=10) ) ]) # bulilding classifier
kernels_c = {'classification__kernel': ['linear','poly','rbf','sigmoid'], 'classification__C': [.001,.01,.1,1,10,100]}
grid_search = GridSearchCV(estimator=SVM_classifier2, param_grid=kernels_c, scoring='precision', cv=5)
grid_search.fit(X_scaled, Y)#training and testing

best_parameters = grid_search.best_params_
print(best_parameters)
best_result = grid_search.best_score_
print(best_result)